{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e463f76d",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x90 in position 106: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fname \u001b[38;5;129;01min\u001b[39;00m extracted_files:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(extract_dir, fname)) \u001b[38;5;28;01mas\u001b[39;00m infile:\n\u001b[1;32m---> 26\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m infile:\n\u001b[0;32m     27\u001b[0m             outfile\u001b[38;5;241m.\u001b[39mwrite(line)\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;66;03m# Ensure there's a newline between files in case the last line of a file doesn't end with one\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m codecs\u001b[38;5;241m.\u001b[39mcharmap_decode(\u001b[38;5;28minput\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors,decoding_table)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x90 in position 106: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "#concatenate all sql files\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "\n",
    "# Define the path for the uploaded zip file and the directory to extract its contents\n",
    "zip_path = \"C:\\\\Users\\\\anany\\\\OneDrive\\\\Desktop\\\\Anti-pattern\\\\Anti-pattern revised.zip\"\n",
    "extract_dir = \"C:\\\\Users\\\\anany\\\\OneDrive\\\\Desktop\\\\Anti-pattern\\\\Anti-pattern\"\n",
    "\n",
    "# Create a directory to extract the zip file contents\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "# Extract the zip file\n",
    "with ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "\n",
    "# List the extracted files to confirm extraction\n",
    "extracted_files = os.listdir(extract_dir)\n",
    "extracted_files\n",
    "# Path for the final concatenated SQL file\n",
    "concatenated_sql_path = 'C:\\\\Users\\\\anany\\\\OneDrive\\\\Desktop\\\\Anti-pattern\\\\concatenated_sql_files.sql'\n",
    "\n",
    "# Concatenate all SQL files into one\n",
    "with open(concatenated_sql_path, 'w') as outfile:\n",
    "    for fname in extracted_files:\n",
    "        with open(os.path.join(extract_dir, fname)) as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)\n",
    "            # Ensure there's a newline between files in case the last line of a file doesn't end with one\n",
    "            outfile.write('\\n')\n",
    "\n",
    "concatenated_sql_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f23bb6e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\anany\\\\OneDrive\\\\Desktop\\\\Anti-pattern\\\\sql_statements.csv'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converting sql to csv\n",
    "import csv\n",
    "\n",
    "# Path for the CSV file to be created\n",
    "csv_path = 'C:\\\\Users\\\\anany\\\\OneDrive\\\\Desktop\\\\Anti-pattern\\\\sql_statements.csv'\n",
    "\n",
    "# Read the SQL file, split statements by semicolon, and write each statement as a row in the CSV\n",
    "with open(concatenated_sql_path, 'r') as sql_file:\n",
    "    sql_content = sql_file.read()\n",
    "\n",
    "# Splitting by semicolon to get individual statements; this assumes that each statement ends with a semicolon\n",
    "sql_statements = sql_content.split(';')\n",
    "\n",
    "# Remove any leading/trailing whitespace and filter out empty statements\n",
    "sql_statements = [stmt.strip() for stmt in sql_statements if stmt.strip()]\n",
    "\n",
    "# Write to CSV\n",
    "with open(csv_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    for statement in sql_statements:\n",
    "        writer.writerow([statement])  # Writing each statement as a row in the CSV\n",
    "\n",
    "csv_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83538654",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/data/Anti_pattern.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load the CSV file into a DataFrame\u001b[39;00m\n\u001b[0;32m      5\u001b[0m csv_path_uploaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/mnt/data/Anti_pattern.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 6\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(csv_path_uploaded, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# Assuming the CSV has a single column and no header\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Remove duplicate rows\u001b[39;00m\n\u001b[0;32m      9\u001b[0m df_unique \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop_duplicates()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1662\u001b[0m     f,\n\u001b[0;32m   1663\u001b[0m     mode,\n\u001b[0;32m   1664\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1665\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1666\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1667\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1668\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1669\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1670\u001b[0m )\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    862\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    863\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/data/Anti_pattern.csv'"
     ]
    }
   ],
   "source": [
    "#removing duplicate value\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "csv_path_uploaded = '/mnt/data/Anti_pattern.csv'\n",
    "df = pd.read_csv(csv_path_uploaded, header=None)  # Assuming the CSV has a single column and no header\n",
    "\n",
    "# Remove duplicate rows\n",
    "df_unique = df.drop_duplicates()\n",
    "\n",
    "# Path for the deduplicated CSV file\n",
    "deduplicated_csv_path = '/mnt/data/anti_pattern_deduplicated.csv'\n",
    "\n",
    "# Save the deduplicated DataFrame to a new CSV file\n",
    "df_unique.to_csv(deduplicated_csv_path, index=False, header=False)\n",
    "\n",
    "deduplicated_csv_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "138cf3be",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_deduplicated' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mSeries([error_exists, error_type, error_location])\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Apply the detailed syntax check function to each SQL statement\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m df_deduplicated[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError_Exists\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError_Type\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError_Location\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m df_deduplicated[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSQL_Statement\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(detailed_sql_syntax_check)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Filter to show only rows with errors for demonstration, though all rows will be saved\u001b[39;00m\n\u001b[0;32m     30\u001b[0m errors_detailed_df \u001b[38;5;241m=\u001b[39m df_deduplicated[df_deduplicated[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError_Exists\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYes\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_deduplicated' is not defined"
     ]
    }
   ],
   "source": [
    "#naturer error findings\n",
    "# Enhance the function to provide more detailed error analysis\n",
    "def detailed_sql_syntax_check(statement):\n",
    "    # Initialize error details\n",
    "    error_exists = 'No'\n",
    "    error_type = ''\n",
    "    error_location = ''\n",
    "\n",
    "    # Check for common SQL start keywords\n",
    "    common_starts = ['SELECT', 'INSERT', 'UPDATE', 'DELETE', 'CREATE', 'ALTER', 'DROP']\n",
    "    if not any(statement.strip().upper().startswith(keyword) for keyword in common_starts):\n",
    "        error_exists = 'Yes'\n",
    "        error_type = 'Uncommon start'\n",
    "        error_location = 'Start of statement'\n",
    "    # check for common sql keywords in the middle\n",
    "    common_middle=['from', 'where','as','and','or']\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Check for mismatched parentheses\n",
    "    open_parentheses = statement.count('(')\n",
    "    close_parentheses = statement.count(')')\n",
    "    if open_parentheses != close_parentheses:\n",
    "        error_exists = 'Yes'\n",
    "        error_type += 'Mismatched parentheses' + ('; ' if error_type else '')\n",
    "        error_location += 'Parentheses count mismatch' + ('; ' if error_location else '')\n",
    "\n",
    "    return pd.Series([error_exists, error_type, error_location])\n",
    "\n",
    "# Apply the detailed syntax check function to each SQL statement\n",
    "df_deduplicated[['Error_Exists', 'Error_Type', 'Error_Location']] = df_deduplicated['SQL_Statement'].apply(detailed_sql_syntax_check)\n",
    "\n",
    "# Filter to show only rows with errors for demonstration, though all rows will be saved\n",
    "errors_detailed_df = df_deduplicated[df_deduplicated['Error_Exists'] == 'Yes']\n",
    "\n",
    "# Define path for the CSV with detailed error analysis\n",
    "detailed_errors_csv_path = '/mnt/data/sql_errors_detailed_analysis.csv'\n",
    "\n",
    "# Save the DataFrame with detailed error analysis to a new CSV file\n",
    "df_deduplicated.to_csv(detailed_errors_csv_path, index=False)\n",
    "\n",
    "detailed_errors_csv_path, errors_detailed_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6dde130",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_deduplicated' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m statement\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Apply the correction function to each SQL statement and create a new column for corrected statements\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m df_deduplicated[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCorrected_SQL_Statement\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_deduplicated[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSQL_Statement\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(correct_sql_statement)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Define path for the CSV with corrected SQL statements\u001b[39;00m\n\u001b[0;32m     25\u001b[0m corrected_sql_csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/mnt/data/sql_corrected_statements.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_deduplicated' is not defined"
     ]
    }
   ],
   "source": [
    "#corrected statements\n",
    "# Function to attempt a basic correction of SQL statements\n",
    "def correct_sql_statement(statement):\n",
    "    # Count the number of opening and closing parentheses\n",
    "    open_parentheses = statement.count('(')\n",
    "    close_parentheses = statement.count(')')\n",
    "\n",
    "    # Attempt to correct mismatched parentheses\n",
    "    while open_parentheses > close_parentheses:\n",
    "        statement += ')'  # Add a closing parenthesis at the end\n",
    "        close_parentheses += 1\n",
    "    while close_parentheses > open_parentheses:\n",
    "        if statement.endswith(')'):\n",
    "            statement = statement[:-1]  # Remove a closing parenthesis from the end\n",
    "            close_parentheses -= 1\n",
    "        else:\n",
    "            break  # Stop if the statement does not end with a closing parenthesis\n",
    "\n",
    "    return statement\n",
    "\n",
    "# Apply the correction function to each SQL statement and create a new column for corrected statements\n",
    "df_deduplicated['Corrected_SQL_Statement'] = df_deduplicated['SQL_Statement'].apply(correct_sql_statement)\n",
    "\n",
    "# Define path for the CSV with corrected SQL statements\n",
    "corrected_sql_csv_path = '/mnt/data/sql_corrected_statements.csv'\n",
    "\n",
    "# Save the DataFrame with corrected SQL statements to a new CSV file\n",
    "df_deduplicated.to_csv(corrected_sql_csv_path, index=False)\n",
    "\n",
    "corrected_sql_csv_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07bc769e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'full_corrected_sql_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#injecting error\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Ensure all SQL statements are treated as strings to prevent the previous error\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m full_corrected_sql_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCorrected_SQL_Statement\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m full_corrected_sql_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCorrected_SQL_Statement\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Re-apply the function to potentially introduce errors into the SQL queries\u001b[39;00m\n\u001b[0;32m      6\u001b[0m full_corrected_sql_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModified_SQL\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError_Type\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError_Location\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m full_corrected_sql_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCorrected_SQL_Statement\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: pd\u001b[38;5;241m.\u001b[39mSeries(maybe_introduce_error(x)))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'full_corrected_sql_df' is not defined"
     ]
    }
   ],
   "source": [
    "#injecting error\n",
    "# Ensure all SQL statements are treated as strings to prevent the previous error\n",
    "full_corrected_sql_df['Corrected_SQL_Statement'] = full_corrected_sql_df['Corrected_SQL_Statement'].astype(str)\n",
    "\n",
    "# Re-apply the function to potentially introduce errors into the SQL queries\n",
    "full_corrected_sql_df[['Modified_SQL', 'Error_Type', 'Error_Location']] = full_corrected_sql_df['Corrected_SQL_Statement'].apply(\n",
    "    lambda x: pd.Series(maybe_introduce_error(x)))\n",
    "\n",
    "# Re-add the column to indicate whether the query is wrong\n",
    "full_corrected_sql_df['Is_Wrong'] = full_corrected_sql_df['Error_Type'].apply(lambda x: 'Yes' if x != 'No modification' else 'No')\n",
    "\n",
    "# Re-define path for the full CSV with randomly introduced errors\n",
    "full_modified_sql_csv_path = '/mnt/data/full_sql_with_random_errors.csv'\n",
    "\n",
    "# Re-save the DataFrame with randomly introduced errors to a new CSV file\n",
    "full_corrected_sql_df.to_csv(full_modified_sql_csv_path, index=False)\n",
    "\n",
    "full_modified_sql_csv_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59339b05",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hj_updated_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m corrected_statement, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYes\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mstr\u001b[39m(e), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCorrected double commas\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Error found and corrected\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Apply the validation and correction function to each SQL statement\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m hj_updated_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCorrected_Statement\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError_Found\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError_Description\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCorrection_Applied\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m hj_updated_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatement\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: pd\u001b[38;5;241m.\u001b[39mSeries(validate_and_correct_sql(x)))\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Display the first few rows to verify the corrections\u001b[39;00m\n\u001b[0;32m     19\u001b[0m hj_updated_df\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hj_updated_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Define a function to validate SQL statements and attempt to correct them if possible\n",
    "def validate_and_correct_sql(statement):\n",
    "    try:\n",
    "        # Pretend to parse the SQL statement to detect errors (simulation, as actual parsing is not feasible here)\n",
    "        # This is a placeholder for actual SQL parsing logic, which would require a proper SQL parser\n",
    "        # For demonstration, we assume the statement is correct and return it as is\n",
    "        return statement, 'No', 'N/A', 'N/A'  # No error found\n",
    "    except Exception as e:\n",
    "        # If an error is detected, attempt to correct it\n",
    "        # This is a simplified correction mechanism and may not cover all SQL syntax errors\n",
    "        corrected_statement = statement.replace(\",,\", \",\")  # Example correction: removing double commas\n",
    "        return corrected_statement, 'Yes', str(e), 'Corrected double commas'  # Error found and corrected\n",
    "\n",
    "# Apply the validation and correction function to each SQL statement\n",
    "hj_updated_df[['Corrected_Statement', 'Error_Found', 'Error_Description', 'Correction_Applied']] = hj_updated_df['statement'].apply(\n",
    "    lambda x: pd.Series(validate_and_correct_sql(x)))\n",
    "\n",
    "# Display the first few rows to verify the corrections\n",
    "hj_updated_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2d0b246",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sqlparse'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#finding the error\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msqlparse\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msqlparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IdentifierList, Identifier\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msqlparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokens\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Keyword, DML\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sqlparse'"
     ]
    }
   ],
   "source": [
    "#finding the error\n",
    "import sqlparse\n",
    "from sqlparse.sql import IdentifierList, Identifier\n",
    "from sqlparse.tokens import Keyword, DML\n",
    "\n",
    "def is_select_statement(parsed):\n",
    "    # This function checks if the parsed statement is a SELECT statement\n",
    "    for token in parsed.tokens:\n",
    "        if token.ttype is DML and token.value.upper() == 'SELECT':\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def find_errors(sql):\n",
    "    # Parse the SQL statement\n",
    "    parsed = sqlparse.parse(sql)[0]\n",
    "    \n",
    "    # Check if it's a SELECT statement\n",
    "    if not is_select_statement(parsed):\n",
    "        return \"Not a SELECT statement\", \"N/A\"\n",
    "    \n",
    "    # Look for common errors like missing FROM clause\n",
    "    from_clause_found = False\n",
    "    for token in parsed.tokens:\n",
    "        if from_clause_found:\n",
    "            break\n",
    "        if isinstance(token, IdentifierList):\n",
    "            for identifier in token.get_identifiers():\n",
    "                if str(identifier).upper().startswith('FROM '):\n",
    "                    from_clause_found = True\n",
    "                    break\n",
    "        elif isinstance(token, Identifier) and str(token).upper().startswith('FROM '):\n",
    "            from_clause_found = True\n",
    "    \n",
    "    if not from_clause_found:\n",
    "        return \"Missing FROM clause\", \"N/A\"\n",
    "    \n",
    "    # No errors found\n",
    "    return \"No errors\", \"N/A\"\n",
    "\n",
    "# Example usage\n",
    "sql_statements = [\n",
    "    \"SELECT * FROM table_name\",  # Correct SQL\n",
    "    \"SELECT *\",  # Missing FROM clause\n",
    "    \"UPDATE table_name SET column=value\"  # Not a SELECT statement\n",
    "]\n",
    "\n",
    "for sql in sql_statements:\n",
    "    error, correction = find_errors(sql)\n",
    "    print(f\"SQL: {sql}\\nError: {error}\\nCorrection: {correction}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52e4ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
